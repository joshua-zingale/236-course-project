{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/03 16:17:07 WARN Utils: Your hostname, MacBook-Air-123.local, resolves to a loopback address: 127.0.0.1; using 10.13.65.202 instead (on interface en0)\n",
      "25/10/03 16:17:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/03 16:17:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as fn\n",
    "import merging\n",
    "import importlib\n",
    "importlib.reload(merging)\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .config(\"spark.sql.ansi.enabled\", \"false\") # Must be disabled because there is an invalid date.\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df = spark.read.csv(\"../data/customer-reservations.csv\", header=True,inferSchema=True)\n",
    "hotel_df = spark.read.csv(\"../data/hotel-booking.csv\", header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------------------+---------+------------+-------------+------------+-------------------+------------------+--------------+\n",
      "|Booking_ID|stays_in_weekend_nights|stays_in_week_nights|lead_time|arrival_year|arrival_month|arrival_date|market_segment_type|avg_price_per_room|booking_status|\n",
      "+----------+-----------------------+--------------------+---------+------------+-------------+------------+-------------------+------------------+--------------+\n",
      "|  INN00001|                      1|                   2|      224|        2017|           10|           2|            Offline|              65.0|  Not_Canceled|\n",
      "|  INN00002|                      2|                   3|        5|        2018|           11|           6|             Online|            106.68|  Not_Canceled|\n",
      "|  INN00003|                      2|                   1|        1|        2018|            2|          28|             Online|              60.0|      Canceled|\n",
      "+----------+-----------------------+--------------------+---------+------------+-------------+------------+-------------------+------------------+--------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "customer_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+---------+------------+-------------+------------------------+-------------------------+-----------------------+--------------------+-------------------+-------+------------------+--------------------+\n",
      "|       hotel|booking_status|lead_time|arrival_year|arrival_month|arrival_date_week_number|arrival_date_day_of_month|stays_in_weekend_nights|stays_in_week_nights|market_segment_type|country|avg_price_per_room|               email|\n",
      "+------------+--------------+---------+------------+-------------+------------------------+-------------------------+-----------------------+--------------------+-------------------+-------+------------------+--------------------+\n",
      "|Resort Hotel|             0|      342|        2015|         July|                      27|                        1|                      0|                   0|             Direct|    PRT|               0.0|Ernest.Barnes31@o...|\n",
      "|Resort Hotel|             0|      737|        2015|         July|                      27|                        1|                      0|                   0|             Direct|    PRT|               0.0|Andrea_Baker94@ao...|\n",
      "|Resort Hotel|             0|        7|        2015|         July|                      27|                        1|                      0|                   1|             Direct|    GBR|              75.0|Rebecca_Parker@co...|\n",
      "+------------+--------------+---------+------------+-------------+------------------------+-------------------------+-----------------------+--------------------+-------------------+-------+------------------+--------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "hotel_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Commonalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('avg_price_per_room', 'double'), ('lead_time', 'int'), ('market_segment_type', 'string'), ('arrival_year', 'int'), ('stays_in_weekend_nights', 'int'), ('stays_in_week_nights', 'int')]\n"
     ]
    }
   ],
   "source": [
    "common_columns, common_types = merging.get_common_columns(customer_df, hotel_df)\n",
    "print(list(zip(common_columns, common_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that\n",
    "\n",
    "```\n",
    "['arrival_year', 'avg_price_per_room', 'stays_in_weekend_nights', 'lead_time', 'stays_in_week_nights', 'market_segment_type']\n",
    "```\n",
    "\n",
    "are shared between both datasets with the same data types and therefore could easily be present in a combined dataset. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Booking_ID', 'string'), ('arrival_date', 'int'), ('booking_status', 'string'), ('arrival_month', 'int')]\n"
     ]
    }
   ],
   "source": [
    "customer_unique_columns, cuc_types = merging.get_left_unique_columns(customer_df, hotel_df)\n",
    "print(list(zip(customer_unique_columns, cuc_types)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('arrival_date_day_of_month', 'int'), ('arrival_month', 'string'), ('hotel', 'string'), ('country', 'string'), ('arrival_date_week_number', 'int'), ('booking_status', 'int'), ('email', 'string')]\n"
     ]
    }
   ],
   "source": [
    "hotel_unique_columns, huc_types = merging.get_left_unique_columns(hotel_df, customer_df)\n",
    "print(list(zip(hotel_unique_columns, huc_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Data\n",
    "\n",
    "Before merging the two data sources into one, they must be normalized into the same schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns that can be dropped\n",
    "Of those unique to `customer_df`,\n",
    "- `Booking_ID` can be dropped since it has no meaning for our purposes.\n",
    "\n",
    "Of those unique to `hotel_df`,\n",
    "- `arrival_date_week_number` can be dropped because it is redundant with the arrival month, date, and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df2 = customer_df.drop(\"Booking_ID\")\n",
    "hotel_df2 = hotel_df.drop(\"arrival_date_week_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns that need transformation before merger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booking status\n",
    "`booking_status`, as revealed from the EDA analysis, looks to be equivalent for both, save that the values are stored under different conventions as given by the following table:\n",
    "\n",
    "| customer_df  | hotel_df |\n",
    "| -----------  | -------- |\n",
    "| Not_Canceled |     0    |\n",
    "|  Canceled    |     1    |\n",
    "\n",
    "I discerned this from how in `customer_df`, about one third of the reservations were canceled.\n",
    "Likewise, about one third of the reservations in `hotel_df` have `1` and the other two thirds `0`,\n",
    "leading me to assume the above mapping.\n",
    "\n",
    "In the merged data, I will convert these into a column called `canceled`, where `0` is \"not canceled\" and `1` is \"canceled\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df2 = customer_df2.withColumn(\n",
    "    \"canceled\",\n",
    "    fn.when(fn.col(\"booking_status\") == \"Not_Canceled\", 0)\n",
    "    .when(fn.col(\"booking_status\") == \"Canceled\", 1)).drop(\"booking_status\")\n",
    "\n",
    "hotel_df2 = hotel_df2.withColumnRenamed(\"booking_status\", \"canceled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrival Time\n",
    "The arrival time is present in both, but with some name and storage type differences.\n",
    "\n",
    "- `arrival_date` in `customer_df` is `arrival_date_day_of_month` in `hotel_df`. \n",
    "- `arrival_month` is stored as an integer 1-12 in `customer_df` but as the full month's name in `hotel_df`.\n",
    "\n",
    "In the merged data, arrival time will be encoded as a single column called `arrival_time` that will store the date of arrival as a `DateType` in PySpark. This will replace the current month, day, and year columns present in both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df2 = (customer_df2\n",
    " .withColumn(\"arrival_time\",\n",
    "             fn.make_date(\n",
    "                 fn.col(\"arrival_year\"),\n",
    "                 fn.col(\"arrival_month\"),\n",
    "                 fn.col(\"arrival_date\")))\n",
    " .drop(\"arrival_year\")\n",
    " .drop(\"arrival_month\")\n",
    " .drop(\"arrival_date\"))\n",
    "\n",
    "\n",
    "def map_mongth_name_to_number(column_name: str):\n",
    "    month_name_to_number = {\n",
    "        \"February\": 2,\n",
    "        \"March\": 3,\n",
    "        \"April\": 4,\n",
    "        \"May\": 5,\n",
    "        \"June\": 6,\n",
    "        \"July\": 7,\n",
    "        \"August\": 8,\n",
    "        \"September\": 9,\n",
    "        \"October\": 10,\n",
    "        \"November\": 11,\n",
    "        \"December\": 12\n",
    "    }\n",
    "    mapping = fn.when(fn.col(column_name) == \"January\", 1)\n",
    "    for name, num in month_name_to_number.items():\n",
    "        mapping = mapping.when(fn.col(column_name) == name, num)\n",
    "    return mapping\n",
    "\n",
    "hotel_df2 = (hotel_df2\n",
    " .withColumn(\"arrival_month\",map_mongth_name_to_number(\"arrival_month\"))\n",
    " .withColumn(\"arrival_time\",\n",
    "             fn.make_date(\n",
    "                 fn.col(\"arrival_year\"),\n",
    "                 fn.col(\"arrival_month\"),\n",
    "                 fn.col(\"arrival_date_day_of_month\")))\n",
    " .drop(\"arrival_year\")\n",
    " .drop(\"arrival_month\")\n",
    " .drop(\"arrival_date_day_of_month\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in Missing Data\n",
    "\n",
    "The `country`, `hotel`, and `email` are present in `hotel_df` but not `customer_df`.\n",
    "These columns may be interesting for our analysis, so I will keep them in the merged data.\n",
    "For the rows that come from `customer_df`, I will insert Null values for these three columns\n",
    "in the merged data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values will automatically be inserted for the rows from customer_df when\n",
    "# the two dataframes are merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concerning New Null Values\n",
    "\n",
    "The built-in data integrity protections of Spark prevented me from processing all of the above operations because it detected a date of February 29th, 2018, which is invalid because 2018 was not a leap year. To permit the operations form continuing, I disabled the data protections by setting `spark.sql.ansi.enabled` to false. After doing this, 37 Null values appeared in the processed version of `customer_df`, which is called `customer_df2`. The code below verifies that all 37 of these nulls result form rows that improperly have\n",
    "February 29th, 2018 as the arrival date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column                     Number of Nulls    \n",
      "------------------------------------------\n",
      "stays_in_weekend_nights    0                  \n",
      "stays_in_week_nights       0                  \n",
      "lead_time                  0                  \n",
      "market_segment_type        0                  \n",
      "avg_price_per_room         0                  \n",
      "canceled                   0                  \n",
      "arrival_time               37                 \n"
     ]
    }
   ],
   "source": [
    "import eda\n",
    "eda.print_num_null_per_column(customer_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.where((fn.col(\"arrival_year\") == 2018) & (fn.col(\"arrival_date\") == 29) & (fn.col(\"arrival_month\") == 2) ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the Data Integrity Issue\n",
    "\n",
    "Since February 29th, 2018 does not exist, I will be fixing the data by replacing the arrival time for all such entries with March 1st, 2018, since that would be the correct day for that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column                     Number of Nulls    \n",
      "------------------------------------------\n",
      "stays_in_weekend_nights    0                  \n",
      "stays_in_week_nights       0                  \n",
      "lead_time                  0                  \n",
      "market_segment_type        0                  \n",
      "avg_price_per_room         0                  \n",
      "canceled                   0                  \n",
      "arrival_time               0                  \n"
     ]
    }
   ],
   "source": [
    "customer_df3 = customer_df2.withColumn(\n",
    "    \"arrival_time\",\n",
    "    fn.coalesce(fn.col(\"arrival_time\"), fn.lit(\"2018-03-01\").cast(\"date\"))\n",
    ")\n",
    "\n",
    "eda.print_num_null_per_column(customer_df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/03 16:17:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 50:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------------+-----------------+-----------------------+--------------------+-------------------+-------+------------------+--------------------+\n",
      "|summary|       hotel|           canceled|        lead_time|stays_in_weekend_nights|stays_in_week_nights|market_segment_type|country|avg_price_per_room|               email|\n",
      "+-------+------------+-------------------+-----------------+-----------------------+--------------------+-------------------+-------+------------------+--------------------+\n",
      "|  count|       78703|             114978|           114978|                 114978|              114978|             114978|  78298|            114978|               78703|\n",
      "|   mean|        NULL| 0.3510584633582077|96.22974829967472|     0.8745499138965716|   2.371088382125276|               NULL|   NULL| 97.80159865365225|                NULL|\n",
      "| stddev|        NULL|0.47730325797274625|100.5264041802159|     0.9546293077588424|  1.7431996147326994|               NULL|   NULL| 44.72740814621265|                NULL|\n",
      "|    min|  City Hotel|                  0|                0|                      0|                   0|           Aviation|    ABW|               0.0|AAdams40@xfinity.com|\n",
      "|    max|Resort Hotel|                  1|              737|                     19|                  50|          Undefined|    ZWE|            5400.0|Zuniga_Thomas@out...|\n",
      "+-------+------------+-------------------+-----------------+-----------------------+--------------------+-------------------+-------+------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- hotel: string (nullable = true)\n",
      " |-- canceled: integer (nullable = true)\n",
      " |-- lead_time: integer (nullable = true)\n",
      " |-- stays_in_weekend_nights: integer (nullable = true)\n",
      " |-- stays_in_week_nights: integer (nullable = true)\n",
      " |-- market_segment_type: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- avg_price_per_room: double (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- arrival_time: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merged_data = hotel_df2.unionByName(customer_df3, allowMissingColumns=True)\n",
    "\n",
    "merged_data.describe().show()\n",
    "merged_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformatting the Merged Data\n",
    "\n",
    "Currently, the number of days in a stay is kept in two columns, `stays_in_weekend_nights` and `stays_in_week_nights`.\n",
    "These should be merged into a single column, `stays_in_nights`, which is the sum of the two.\n",
    "This will not lead to a loss of information because whether a stay was on a weeknight or not can be deduced from the\n",
    "arrival date in conjunction with the number of nights stayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data2 = (merged_data\n",
    "                .withColumn(\n",
    "                    \"stays_in_nights\",\n",
    "                    fn.col(\"stays_in_weekend_nights\") + fn.col(\"stays_in_week_nights\"))\n",
    "                .drop(\"stays_in_weekend_nights\")\n",
    "                .drop(\"stays_in_week_nights\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    tmpdir = Path(tmpdir) / \"unified\"\n",
    "    merged_data2.repartition(1).write.csv(str(tmpdir), header=True)\n",
    "    csv_file = tmpdir / next(filter(lambda p: p.suffix == \".csv\", map(Path, os.listdir(tmpdir))))\n",
    "    shutil.copy(csv_file, Path(\"../\", \"data\", \"unified.csv\"))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "236-course-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
